{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnNimWh3t/b7uwye2o74J5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamnegi92/GenAI-Playlist/blob/main/Agents/Agentic_Deep_Researcher_HF%2C_Mistral_%2C_Langraph%2C_gpu_poor_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers accelerate huggingface_hub langchain duckduckgo-search langchain-community langgraph  auto-gptq optimum langchain-community\n"
      ],
      "metadata": {
        "id": "-qLIxZcDrSQ4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS5oNhcrrMm-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TaN9xgawrIp5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "xjOLFS-tqek5",
        "outputId": "f6f67fa9-a0b5-4abb-aea5-a22981f7ffac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'duckduckgo_search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-74235666.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mduckduckgo_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'duckduckgo_search'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import time\n",
        "from typing import Dict, Any, List, TypedDict\n",
        "\n",
        "import torch\n",
        "from duckduckgo_search import DDGS\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.tools import tool\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.schema.runnable import Runnable\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# ----------- Web Search Tool -----------\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Search the web for recent info about a topic using DuckDuckGo.\"\"\"\n",
        "    results = DDGS().text(query, max_results=3)\n",
        "    summaries = [f\"{r['title']}: {r['body'][:150]}... ({r['href']})\" for r in results]\n",
        "    return \"\\n\".join(summaries)\n",
        "\n",
        "\n",
        "# ----------- HF Chat Model Wrapper -----------\n",
        "class HFChatModel(Runnable):\n",
        "    def __init__(self):\n",
        "        model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    def invoke(self, messages, config=None):\n",
        "        prompt = \"\\n\".join([\n",
        "            f\"User: {msg.content}\" if isinstance(msg, HumanMessage)\n",
        "            else f\"Assistant: {msg.content}\"\n",
        "            for msg in messages\n",
        "        ])\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                top_p=0.9\n",
        "            )\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        reply = decoded[len(prompt):].strip().split(\"Assistant:\")[-1].strip()\n",
        "        return AIMessage(content=reply)\n",
        "\n",
        "\n",
        "llm = HFChatModel()\n",
        "\n",
        "\n",
        "# ----------- LangGraph Node Functions -----------\n",
        "def planner_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state[\"messages\"]\n",
        "    prompt = \"List exactly 2 short steps to research this topic. Keep each step concise (1 sentence).\"\n",
        "    print(f\"\\n[ðŸ§  Planner Prompt]: {prompt}\")\n",
        "    response = llm.invoke(messages + [HumanMessage(content=prompt)])\n",
        "    print(f\"[ðŸ“‹ Planner Output]: {response.content[:300]}\")\n",
        "    return {\"messages\": messages + [response]}\n",
        "\n",
        "\n",
        "def researcher_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state[\"messages\"]\n",
        "    last_instruction = messages[-1].content\n",
        "    query = last_instruction.split(\"\\n\")[0].strip(\"-â€¢123. \")\n",
        "    print(f\"\\n[ðŸ” Researcher running web_search on]: {query}\")\n",
        "    result = web_search.invoke(query)\n",
        "    print(f\"[ðŸ“„ Research Result Snippet]:\\n{result[:300]}...\\n\")\n",
        "    return {\"messages\": add_messages(messages, HumanMessage(content=f\"Web result for '{query}': {result}\"))}\n",
        "\n",
        "\n",
        "def writer_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state[\"messages\"]\n",
        "    prompt = \"Write a 1-paragraph summary based on the research above. Focus only on key findings.\"\n",
        "    print(f\"\\n[âœï¸ Writer Prompt]: {prompt}\")\n",
        "    response = llm.invoke(messages + [HumanMessage(content=prompt)])\n",
        "    print(f\"[ðŸ“ Final Report Preview]: {response.content[:300]}\")\n",
        "    return {\"messages\": messages + [response]}\n",
        "\n",
        "\n",
        "# ----------- LangGraph Definition -----------\n",
        "class GraphState(TypedDict):\n",
        "    messages: List[Any]\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"Planner\", planner_node)\n",
        "workflow.add_node(\"Researcher\", researcher_node)\n",
        "workflow.add_node(\"Writer\", writer_node)\n",
        "workflow.set_entry_point(\"Planner\")\n",
        "workflow.add_edge(\"Planner\", \"Researcher\")\n",
        "workflow.add_edge(\"Researcher\", \"Writer\")\n",
        "workflow.set_finish_point(\"Writer\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "# ----------- Get Topic Safely -----------\n",
        "try:\n",
        "    import sys\n",
        "    if len(sys.argv) > 1 and not sys.argv[1].startswith(\"-\"):\n",
        "        topic = sys.argv[1]\n",
        "    else:\n",
        "        topic = input(\"Enter your research topic: \").strip() or \"Impacts of generative AI in healthcare\"\n",
        "except:\n",
        "    topic = input(\"Enter your research topic: \").strip() or \"Impacts of generative AI in healthcare\"\n",
        "\n",
        "\n",
        "# ----------- Run the Agent -----------\n",
        "print(f\"\\nðŸš€ Starting research on: {topic}\\n\")\n",
        "\n",
        "state = {\"messages\": [HumanMessage(content=f\"Research topic: {topic}\")]}\n",
        "final_state = graph.invoke(state)\n",
        "\n",
        "final_report = final_state[\"messages\"][-1].content\n",
        "print(\"\\nâœ… DONE! Final Report:\\n\")\n",
        "print(final_report)\n"
      ]
    }
  ]
}