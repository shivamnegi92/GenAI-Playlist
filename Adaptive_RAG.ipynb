{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPWl/+6dJEmD88idmg+Aba"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Adaptive RAG\n",
        "```\n",
        "Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\n",
        "\n",
        "In the paper, they report query analysis to route across:\n",
        "\n",
        "No Retrieval\n",
        "Single-shot RAG\n",
        "Iterative RAG\n",
        "Let's build on this using LangGraph.\n",
        "\n",
        "In our implementation, we will route between:\n",
        "\n",
        "Web search: for questions related to recent events\n",
        "Self-corrective RAG: for questions related to our index```\n",
        "\n"
      ],
      "metadata": {
        "id": "-kcmH3i7DjwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B3WlpaCzD3CD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug9B9EqCcQ71",
        "outputId": "2d17a177-fb07-44a6-9640-127ceb7bda65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-cohere in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/dist-packages (0.1.21)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.7.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.86.0)\n",
            "Requirement already satisfied: cohere<6.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (5.15.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (2.11.7)\n",
            "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (6.0.12.20250516)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.4.20250611)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (1.11.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.33.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_community tiktoken langchain-huggingface chromadb langchain langgraph tavily-python sentence_transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXgehZPKdyYK",
        "outputId": "320b875a-5018-446f-c90e-d10c6c3d23fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.7.8)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLgHOzHBumKO",
        "outputId": "1ca63aee-25b3-448a-d2cb-45eec6a81204"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.66)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings # Import Hugging Face Embeddings\n",
        "\n",
        "# Set embeddings using a Hugging Face model\n",
        "# You can replace \"sentence-transformers/all-mpnet-base-v2\" with another model if desired [1]\n",
        "embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Docs to index\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embd,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A44Lu9MmcVUQ",
        "outputId": "d5f04600-d4da-4f99-af86-27d7fc9291c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "# Pydantic output schema\n",
        "class RouteQuery(BaseModel):\n",
        "    datasource: Literal[\"vectorstore\", \"web_search\"]\n",
        "\n",
        "# Get token and configure LLM\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = ChatHuggingFace(\n",
        "    llm=HuggingFaceEndpoint(\n",
        "        repo_id=model_id,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=hf_token,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Escape JSON braces with double {{...}} to prevent LangChain parsing error\n",
        "system_prompt = \"\"\"You are a routing classifier.\n",
        "Reply with ONLY one of these JSON objects:\n",
        "{{\"datasource\": \"vectorstore\"}}\n",
        "{{\"datasource\": \"web_search\"}}\n",
        "\n",
        "Use 'vectorstore' for questions about agents, prompt engineering, or adversarial attacks.\n",
        "Use 'web_search' for everything else.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "# Extract and validate JSON from model output\n",
        "def parse_response(output_text: str) -> RouteQuery:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        return RouteQuery.model_validate(eval(raw_json))  # Safe only if model is trusted\n",
        "    except (ValidationError, ValueError, SyntaxError) as e:\n",
        "        print(\" Failed to parse response:\", output_text)\n",
        "        return RouteQuery(datasource=\"web_search\")\n",
        "\n",
        "# Wrapper to classify user query\n",
        "def classify(question: str) -> RouteQuery:\n",
        "    response = chain.invoke({\"question\": question})\n",
        "    text = response.content.strip() if hasattr(response, \"content\") else str(response)\n",
        "    return parse_response(text)\n",
        "\n",
        "# Run test cases\n",
        "print(classify(\"What are the types of agent memory?\"))\n",
        "print(classify(\"Who will the Bears draft first in the NFL draft?\"))\n"
      ],
      "metadata": {
        "id": "Zhg6pqUDccCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Define schema\n",
        "class GradeDocuments(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"]\n",
        "\n",
        "# Step 2: Initialize Hugging Face LLM\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = ChatHuggingFace(\n",
        "    llm=HuggingFaceEndpoint(\n",
        "        repo_id=model_id,\n",
        "        task=\"text-generation\",\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "        huggingfacehub_api_token=hf_token,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Create prompt (escape braces)\n",
        "system_msg = \"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
        "\n",
        "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
        "\n",
        "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "\n",
        "Reply ONLY with one of these JSON objects:\n",
        "{{\"binary_score\": \"yes\"}}\n",
        "{{\"binary_score\": \"no\"}}\n",
        "\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg),\n",
        "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
        "])\n",
        "\n",
        "grading_chain = grade_prompt | llm\n",
        "\n",
        "# Step 4: Parse the JSON safely\n",
        "def parse_grade_output(output_text: str) -> GradeDocuments:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        return GradeDocuments.model_validate(eval(raw_json))\n",
        "    except (ValidationError, ValueError, SyntaxError):\n",
        "        print(\" Failed to parse:\", output_text)\n",
        "        return GradeDocuments(binary_score=\"no\")\n",
        "\n",
        "# Step 5: Wrap end-to-end grader\n",
        "def evaluate_document_relevance(question: str, document: str) -> GradeDocuments:\n",
        "    response = grading_chain.invoke({\"question\": question, \"document\": document})\n",
        "    text = response.content.strip() if hasattr(response, \"content\") else str(response)\n",
        "    return parse_grade_output(text)\n",
        "\n",
        "# Step 6: Example run\n",
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(evaluate_document_relevance(question, doc_txt))\n"
      ],
      "metadata": {
        "id": "7iSG3-7Zwe-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate based on Chosen Route - RAG vs WebSearch"
      ],
      "metadata": {
        "id": "7u84nER5ssSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "#  Token\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "#  Hugging Face chat model setup\n",
        "llm = ChatHuggingFace(\n",
        "    llm=HuggingFaceEndpoint(\n",
        "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  # or another chat-compatible model\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=hf_token,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.1,\n",
        "    )\n",
        ")\n",
        "\n",
        "#  Prompt from LangChain hub (rag-style)\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "#  Output parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "#  Combine into chain\n",
        "rag_chain = prompt | llm | parser\n",
        "\n",
        "#  Helper to format retrieved docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "#  Test invocation\n",
        "question = \"What types of memory do LLM agents use?\"\n",
        "docs = retriever.invoke(question)\n",
        "formatted = format_docs(docs)\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": formatted, \"question\": question})\n",
        "print(generation)\n"
      ],
      "metadata": {
        "id": "d5m-tW_LfqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hallucination Grader"
      ],
      "metadata": {
        "id": "1sSePBHM2TNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "#  Schema for hallucination grader\n",
        "class GradeHallucinations(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "#  Load HF token and model\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = ChatHuggingFace(\n",
        "    llm=HuggingFaceEndpoint(\n",
        "        repo_id=model_id,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=hf_token,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "    )\n",
        ")\n",
        "\n",
        "#  Prompt (with escaped braces for LangChain templating)\n",
        "system_msg = \"\"\"\n",
        "You are a grader assessing whether an LLM generation is grounded in a set of retrieved facts.\n",
        "Respond with ONLY one of the following JSON objects:\n",
        "{{\"binary_score\": \"yes\"}} or {{\"binary_score\": \"no\"}}\n",
        "\n",
        "'yes' means the answer is grounded in the facts. 'no' means it contains unsupported information.\n",
        "\"\"\"\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg),\n",
        "    (\"human\", \"Set of facts:\\n\\n{documents}\\n\\nLLM generation:\\n\\n{generation}\")\n",
        "])\n",
        "\n",
        "grader_chain = hallucination_prompt | llm\n",
        "\n",
        "#  Parse model response using regex + Pydantic\n",
        "def parse_hallucination_output(output_text: str) -> GradeHallucinations:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        return GradeHallucinations.model_validate(eval(raw_json))\n",
        "    except (ValidationError, ValueError, SyntaxError):\n",
        "        print(\" Failed to parse:\", output_text)\n",
        "        return GradeHallucinations(binary_score=\"no\")\n",
        "\n",
        "#  Wrapper\n",
        "def evaluate_hallucination(documents: str, generation: str) -> GradeHallucinations:\n",
        "    response = grader_chain.invoke({\"documents\": documents, \"generation\": generation})\n",
        "    text = response.content.strip() if hasattr(response, \"content\") else str(response)\n",
        "    return parse_hallucination_output(text)\n",
        "\n",
        "#  Example usage\n",
        "documents_str = \"\\n\\n\".join(doc.page_content for doc in docs)  # Convert list[Document] to str\n",
        "result = evaluate_hallucination(documents_str, generation)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "XzcItyU70-Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Judge- Answer addresses the question"
      ],
      "metadata": {
        "id": "V5onp_Et4A1c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "at-rswZ-4Ahe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "#  Schema\n",
        "class GradeAnswer(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "#  Prompt\n",
        "system_msg = \"\"\"\n",
        "You are a grader assessing whether an answer addresses a user question.\n",
        "Respond with ONLY one of these JSON objects:\n",
        "{{\"binary_score\": \"yes\"}} or {{\"binary_score\": \"no\"}}\n",
        "\n",
        "'yes' means the answer directly addresses the question. 'no' means it does not.\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg),\n",
        "    (\"human\", \"User question:\\n\\n{question}\\n\\nLLM generation:\\n\\n{generation}\")\n",
        "])\n",
        "\n",
        "#  Chain\n",
        "grader_chain = answer_prompt | llm\n",
        "\n",
        "def parse_answer_output(output_text: str) -> GradeAnswer:\n",
        "    try:\n",
        "        # extract first JSON block\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON found\")\n",
        "        json_str = match.group(0)\n",
        "        parsed = json.loads(json_str)\n",
        "        return GradeAnswer.model_validate(parsed)\n",
        "    except (ValidationError, ValueError, SyntaxError, json.JSONDecodeError) as e:\n",
        "        print(\" Failed to parse:\", output_text)\n",
        "        return GradeAnswer(binary_score=\"no\")\n",
        "\n",
        "#  Wrapper\n",
        "def evaluate_answer_relevance(question: str, generation: str) -> GradeAnswer:\n",
        "    response = grader_chain.invoke({\"question\": question, \"generation\": generation})\n",
        "    if hasattr(response, \"content\"):\n",
        "        text = response.content.strip()\n",
        "    else:\n",
        "        text = str(response).strip()\n",
        "    return parse_answer_output(text)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Retrieved docs\n",
        "docs = retriever.invoke(question)\n",
        "\n",
        "formatted_context = format_docs(docs)\n",
        "\n",
        "generation = rag_chain.invoke({\"context\": formatted_context, \"question\": question})\n",
        "result = evaluate_answer_relevance(question, generation)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "BTaxIH5t2RtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Re-writer\n"
      ],
      "metadata": {
        "id": "bvog3jYy5pgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "#  Prompt Template\n",
        "system_msg = \"\"\"\n",
        "You are a question rewriter that converts an input question into a more effective version\n",
        "for vectorstore retrieval. Your goal is to optimize the query for semantic intent and relevant keywords.\n",
        "\"\"\"\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg),\n",
        "    (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nRewrite it for semantic search.\")\n",
        "])\n",
        "\n",
        "#  Chain: prompt  LLM  plain text parser\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "\n",
        "improved = question_rewriter.invoke({\"question\": question})\n",
        "print(\" Rewritten:\", improved)\n"
      ],
      "metadata": {
        "id": "M3xqZIgt4hOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Websearch Agent"
      ],
      "metadata": {
        "id": "aT8OHIeA_EUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U langchain-tavily\n"
      ],
      "metadata": {
        "id": "f1LRM5-I6FEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Search\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load securely from secrets if available\n",
        "api_key = userdata.get(\"TAVILY_TOKEN\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing TAVILY_API_KEY in Colab secrets.\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = api_key\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "FeZ0H5td5nir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# web_search_tool = TavilySearchResults(k=3)  # Works once key is set\n",
        "# print(web_search_tool.invoke(\"Who is the CEO of OpenAI?\"))\n"
      ],
      "metadata": {
        "id": "t_BBOkdL51BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct the Graph\n",
        "#### Capture the flow in as a graph.\n",
        "\n",
        "Define Graph State"
      ],
      "metadata": {
        "id": "sKFxMHHE_PyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "at7gij6l7LLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Graph Flow\n",
        "###  API Reference: [Document]"
      ],
      "metadata": {
        "id": "xSGJuIW3_euP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pydantic output schema for Routing\n",
        "class RouteQuery(BaseModel):\n",
        "    datasource: Literal[\"vectorstore\", \"web_search\"]\n",
        "\n",
        "\n",
        "# Question Router\n",
        "system_prompt = \"\"\"You are a routing classifier.\n",
        "Reply with ONLY one of these JSON objects:\n",
        "{{\"datasource\": \"vectorstore\"}}\n",
        "{{\"datasource\": \"web_search\"}}\n",
        "\n",
        "Use 'vectorstore' for questions about agents, prompt engineering, or adversarial attacks.\n",
        "Use 'web_search' for everything else.\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "# The chain definition itself is fine\n",
        "question_router = prompt | llm\n",
        "\n",
        "# Retrieval Grader (Relevance)\n",
        "class GradeDocuments(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"]\n",
        "\n",
        "system_msg_retrieval = \"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
        "\n",
        "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
        "\n",
        "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "\n",
        "Reply ONLY with one of these JSON objects:\n",
        "{{\"binary_score\": \"yes\"}}\n",
        "{{\"binary_score\": \"no\"}}\n",
        "\"\"\"\n",
        "grade_prompt_retrieval = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg_retrieval),\n",
        "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
        "])\n",
        "retrieval_grader = grade_prompt_retrieval | llm\n",
        "\n",
        "\n",
        "# Hallucination Grader\n",
        "class GradeHallucinations(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "system_msg_hallucination = \"\"\"\n",
        "You are a grader assessing whether an LLM generation is grounded in a set of retrieved facts.\n",
        "Respond with ONLY one of the following JSON objects:\n",
        "{{\"binary_score\": \"yes\"}} or {{\"binary_score\": \"no\"}}\n",
        "\n",
        "'yes' means the answer is grounded in the facts. 'no' means it contains unsupported information.\n",
        "\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg_hallucination),\n",
        "    (\"human\", \"Set of facts:\\n\\n{documents}\\n\\nLLM generation:\\n\\n{generation}\")\n",
        "])\n",
        "hallucination_grader = hallucination_prompt | llm\n",
        "\n",
        "\n",
        "# Answer Grader\n",
        "class GradeAnswer(BaseModel):\n",
        "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "system_msg_answer = \"\"\"\n",
        "You are a grader assessing whether an answer addresses a user question.\n",
        "Respond with ONLY one of these JSON objects:\n",
        "{{\"binary_score\": \"yes\"}} or {{\"binary_score\": \"no\"}}\n",
        "\n",
        "'yes' means the answer directly addresses the question. 'no' means it does not.\n",
        "\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg_answer),\n",
        "    (\"human\", \"User question:\\n\\n{question}\\n\\nLLM generation:\\n\\n{generation}\")\n",
        "])\n",
        "answer_grader = answer_prompt | llm\n",
        "\n",
        "# --- Parsing Functions with improved error handling ---\n",
        "\n",
        "def parse_route_output(output_text: str) -> RouteQuery:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        # Using json.loads is generally safer than eval\n",
        "        parsed = json.loads(raw_json)\n",
        "        return RouteQuery.model_validate(parsed)\n",
        "    except (ValidationError, ValueError, SyntaxError, json.JSONDecodeError) as e:\n",
        "        print(f\" Failed to parse RouteQuery response: {e}\")\n",
        "        print(\"Raw output text:\", output_text) # Print raw output on failure\n",
        "        return RouteQuery(datasource=\"web_search\") # Fallback\n",
        "\n",
        "\n",
        "def parse_grade_output(output_text: str) -> GradeDocuments:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        parsed = json.loads(raw_json) # Using json.loads\n",
        "        return GradeDocuments.model_validate(parsed)\n",
        "    except (ValidationError, ValueError, SyntaxError, json.JSONDecodeError) as e:\n",
        "        print(f\" Failed to parse GradeDocuments response: {e}\")\n",
        "        print(\"Raw output text:\", output_text) # Print raw output on failure\n",
        "        return GradeDocuments(binary_score=\"no\") # Fallback\n",
        "\n",
        "def parse_hallucination_output(output_text: str) -> GradeHallucinations:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON object found.\")\n",
        "        raw_json = match.group(0)\n",
        "        parsed = json.loads(raw_json) # Using json.loads\n",
        "        return GradeHallucinations.model_validate(parsed)\n",
        "    except (ValidationError, ValueError, SyntaxError, json.JSONDecodeError) as e:\n",
        "        print(f\" Failed to parse GradeHallucinations response: {e}\")\n",
        "        print(\"Raw output text:\", output_text) # Print raw output on failure\n",
        "        return GradeHallucinations(binary_score=\"no\") # Fallback\n",
        "\n",
        "def parse_answer_output(output_text: str) -> GradeAnswer:\n",
        "    try:\n",
        "        match = re.search(r\"{.*}\", output_text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON found\")\n",
        "        json_str = match.group(0)\n",
        "        parsed = json.loads(json_str)\n",
        "        return GradeAnswer.model_validate(parsed)\n",
        "    except (ValidationError, ValueError, SyntaxError, json.JSONDecodeError) as e:\n",
        "        print(f\" Failed to parse GradeAnswer response: {e}\")\n",
        "        print(\"Raw output text:\", output_text) # Print raw output on failure\n",
        "        return GradeAnswer(binary_score=\"no\") # Fallback\n",
        "\n",
        "\n",
        "# --- Node Functions (Keep these largely the same, ensuring they use the correct graders/routers) ---\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Format documents for the rag_chain context\n",
        "    formatted_context = format_docs(documents)\n",
        "\n",
        "    # RAG generation (rag_chain defined elsewhere, assuming it uses the correct llm)\n",
        "    generation = rag_chain.invoke({\n",
        "        \"context\": formatted_context,\n",
        "        \"question\": question\n",
        "    })\n",
        "    # Return the updated state\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        raw_score_output = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        # Use the updated parse_grade_output function\n",
        "        score = parse_grade_output(raw_score_output.content if hasattr(raw_score_output, 'content') else str(raw_score_output))\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question (question_rewriter needs to be defined elsewhere, assuming it exists)\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    # Ensure better_question is a string\n",
        "    better_question = better_question.content if hasattr(better_question, 'content') else str(better_question)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "    Returns:\n",
        "        dict: Updated state with documents\n",
        "    \"\"\"\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "\n",
        "    if isinstance(docs, list):\n",
        "        # TavilySearchResults can return a list of dictionaries\n",
        "        results = [Document(page_content=d.get(\"content\", \"\")) for d in docs] # Use .get for safety\n",
        "    else:\n",
        "        # Handle cases where it might return a single result or unexpected format\n",
        "        content = docs.get(\"content\", \"\") if isinstance(docs, dict) else str(docs)\n",
        "        results = [Document(page_content=content)]\n",
        "\n",
        "    return {\"documents\": results, \"question\": question}\n",
        "\n",
        "\n",
        "# --- Edge Functions (Focus on route_question error handling) ---\n",
        "\n",
        "def route_question(state):\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    raw = None\n",
        "    try:\n",
        "        # Invoke the router chain\n",
        "        raw = question_router.invoke({\"question\": question})\n",
        "        # Attempt to parse the response\n",
        "        # Use parse_route_output function\n",
        "        route_decision = parse_route_output(raw.content if hasattr(raw, 'content') else str(raw))\n",
        "        decision = route_decision.datasource\n",
        "        print(f\"---ROUTE DECISION: {decision}---\")\n",
        "        return decision\n",
        "    except Exception as e:\n",
        "        print(f\" Error during question routing: {e}\")\n",
        "        print(f\"Raw output from router (if available): {raw}\")\n",
        "        # Fallback to web search on any error during routing\n",
        "        print(\"---FALLBACK: WEB SEARCH---\")\n",
        "        return \"web_search\"\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "    \"\"\"\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    # Ensure documents key exists and is a list\n",
        "    filtered_documents = state.get(\"documents\", [])\n",
        "\n",
        "    if not filtered_documents or not isinstance(filtered_documents, list) or all(not isinstance(d, Document) for d in filtered_documents):\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT OR DOCUMENTS ARE INVALID, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    # Ensure documents is in a format the grader expects (e.g., a string of content)\n",
        "    documents_content = \"\\n\\n\".join([d.page_content for d in documents if isinstance(d, Document)])\n",
        "\n",
        "    raw_hallucination_score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents_content, \"generation\": generation}\n",
        "    )\n",
        "    # Use the updated parse_hallucination_output function\n",
        "    hallucination_score = parse_hallucination_output(raw_hallucination_score.content if hasattr(raw_hallucination_score, 'content') else str(raw_hallucination_score))\n",
        "    hallucination_grade = hallucination_score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if hallucination_grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        raw_answer_score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        # Use the updated parse_answer_output function\n",
        "        answer_score = parse_answer_output(raw_answer_score.content if hasattr(raw_answer_score, 'content') else str(raw_answer_score))\n",
        "        answer_grade = answer_score.binary_score\n",
        "\n",
        "        if answer_grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\"\n"
      ],
      "metadata": {
        "id": "pjfe3eIB_N9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile Graph\n",
        "API Reference: END | StateGraph | START"
      ],
      "metadata": {
        "id": "ZxGyxM5qAoIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[Document] # Ensure documents is List[Document]\n",
        "\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"web_search\", \"generate\") # Web search results go directly to generate\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\") # Retrieval goes to grading\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate, # Decide based on grading\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\", # If no relevant docs, transform query\n",
        "        \"generate\": \"generate\", # If relevant docs, generate\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\") # After transforming query, retrieve again\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question, # Grade the generated answer\n",
        "    {\n",
        "        \"not supported\": \"generate\", # If not grounded, try generating again (potentially with same docs)\n",
        "        \"useful\": END, # If grounded and answers question, finish\n",
        "        \"not useful\": \"transform_query\", # If grounded but doesn't answer question, transform query\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the updated workflow\n",
        "app: CompiledStateGraph = workflow.compile() # Explicitly type the compiled graph\n"
      ],
      "metadata": {
        "id": "IjzIllfX_bNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Graph\n"
      ],
      "metadata": {
        "id": "HRpZtPC_AtWT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taBxd5vs_10v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
        "rag_chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "system_msg_rewrite = \"\"\"\n",
        "You are a question rewriter that converts an input question into a more effective version\n",
        "for vectorstore retrieval. Your goal is to optimize the query for semantic intent and relevant keywords.\n",
        "\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg_rewrite),\n",
        "    (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nRewrite it for semantic search.\")\n",
        "])\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lX8xQq7Q_39-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
        "final_generation = None\n",
        "\n",
        "print(\"--- STARTING GRAPH RUN ---\")\n",
        "try:\n",
        "    for step in app.stream(inputs):\n",
        "        for node_name, node_output in step.items():\n",
        "            pprint(f\" Node: {node_name}\")\n",
        "            pprint(node_output)\n",
        "\n",
        "            # Capture the generation if it exists in the node output\n",
        "            if isinstance(node_output, dict) and \"generation\" in node_output:\n",
        "                final_generation = node_output[\"generation\"]\n",
        "        print(\"\\n---\")\n",
        "\n",
        "    if final_generation:\n",
        "        print(\" Final Generation:\")\n",
        "        pprint(final_generation)\n",
        "    else:\n",
        "        print(\" No generation found in any step.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred during the graph execution: {e}\")\n"
      ],
      "metadata": {
        "id": "1nG8igQ8BCrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlzYSt46DIIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}